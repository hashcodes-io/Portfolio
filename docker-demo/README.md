# ğŸ”¥ Real-Time Data Pipeline with Kafka, Logstash, Elasticsearch, and Kibana

This project demonstrates a **complete real-time data pipeline** built using **Kafka**, **Logstash**, **Elasticsearch**, and **Kibana** â€” powered by **Docker Compose**.  
A lightweight **Python data generator** continuously pushes simulated JSON events to Kafka, which Logstash consumes and indexes into Elasticsearch for real-time visualization in Kibana.

---

## âš¡ TL;DR â€” Portfolio Projects Overview

1. **Elasticsearch + Kibana (Docker Compose)** â€“ Containerized stack for real-time data monitoring, search, and interactive dashboards.  
2. **Kafka + Logstash + Data Generator (Docker Compose)** â€“ End-to-end streaming pipeline that ingests, processes, and prepares data for analytics or ML workflows.

---

ğŸ“ **In short:**  
> I design and deploy **data-driven applications** that combine **APIs and streaming systems**, using **Docker + FastAPI**.

## ğŸ—ï¸ Architecture

```bash
Data Generator â†’ Kafka â†’ Logstash â†’ Elasticsearch â†’ Kibana
```

### Components

| Service | Description |
|----------|-------------|
| **Zookeeper** | Coordinates Kafka brokers |
| **Kafka** | Message queue that stores event streams |
| **Logstash** | Consumes Kafka messages and indexes them into Elasticsearch |
| **Elasticsearch** | Stores and indexes incoming data |
| **Kibana** | Visualization dashboard for Elasticsearch data |
| **Data Generator** | Python script that sends random JSON events to Kafka |

---

## ğŸ“‚ Project Structure

```bash
docker-demo/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ logstash/
â”‚ â””â”€â”€ logstash.conf
â”œâ”€â”€ elasticsearch/
â”‚ â””â”€â”€ elasticsearch.yml
â”œâ”€â”€ kibana/
â”‚ â””â”€â”€ kibana.yml
â”œâ”€â”€ data-generator/
â”‚ â””â”€â”€ producer.py
â”‚ â””â”€â”€ requirements.txt
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ fastapi-app/
â”‚ â””â”€â”€ main.py
â”‚ â””â”€â”€ requirements.txt
â”‚ â””â”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Start the Pipeline

Run the full stack using Docker Compose:

```bash
docker-compose up -d
```

This will spin up:

- Zookeeper
- Kafka
- Logstash
- Elasticsearch
- Kibana
- data-generator
- fastapi-app

ğŸ•“ Wait about 30â€“60 seconds for all services to initialize.


### 2ï¸âƒ£ Create the Kafka Topic
(Optional â€” Logstash will create it automatically on most setups)
```bash
docker exec -it kafka kafka-topics --create --topic raw-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1
```

### 3ï¸âƒ£ View Data in Kibana

- Open http://localhost:5601 
- Navigate to Stack Management â†’ Index Patterns
- Create a new pattern
- kafka-data-*

Explore your live data in Discover or create visualizations.

### 4ï¸âƒ£ Play with FastApi

- Open http://localhost:8000/docs
- Open POST tab
- Click "Try it Out" button on the right and go crazy 

---

## ğŸ§¹ Cleanup

To stop and remove all containers:

```bash
docker-compose down
```

To remove all data volumes as well:
```bash
docker-compose down -v
```
---

## âš ï¸ Troubleshooting

| Issue                                     | Solution                                                                                    |
| ----------------------------------------- | ------------------------------------------------------------------------------------------- |
| **Kibana canâ€™t connect to Elasticsearch** | Ensure `xpack.security.enabled=false` in `elasticsearch.yml`                                |
| **Producer canâ€™t connect to Kafka**       | Ensure you use `localhost:9092` for local Python scripts, `kafka:9092` inside Docker        |
| **Data not showing in Kibana**            | Wait for Logstash to consume messages or check Logstash logs with `docker logs -f logstash` |


---

## ğŸ“œ License

Created for educational and development purposes.

---

## âœ¨ Author

Hashaam Ahsan
hashaamahsan@gmail.com

